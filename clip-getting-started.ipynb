{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%bash\nconda install --yes -c pytorch pytorch=1.7.1 torchvision cudatoolkit=11.0\npip install ftfy regex tqdm\npip install git+https://github.com/openai/CLIP.git","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport clip\nfrom PIL import Image\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"ViT-B/32\", device=device)\n\nimage = preprocess(Image.open(\"../input/clip-getting-started-dataset/pexels-artem-beliaikin-1485637.jpg\")).unsqueeze(0).to(device)\ntext = clip.tokenize([\"a photo of a dog\", \"a photo of a cat\", \"a photo of a tiger\"]).to(device)\n\nwith torch.no_grad():\n    image_features = model.encode_image(image)\n    text_features = model.encode_text(text)\n    \n    logits_per_image, logits_per_text = model(image, text)\n    probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n\nprint(\"Label probs:\", probs)  # prints: [[0.9927937  0.00421068 0.00299572]]","metadata":{"execution":{"iopub.status.busy":"2021-06-20T21:18:04.80434Z","iopub.execute_input":"2021-06-20T21:18:04.804993Z","iopub.status.idle":"2021-06-20T21:18:08.889586Z","shell.execute_reply.started":"2021-06-20T21:18:04.804937Z","shell.execute_reply":"2021-06-20T21:18:08.888512Z"},"trusted":true},"execution_count":null,"outputs":[]}]}